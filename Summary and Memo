# Memo: Thinking Behind the Build  
### Project: “Determining Which Military Branch of Service to Join — and Why”

This memo explains the thinking, design decisions, and ethical reasoning behind the AI-powered project developed to help individuals explore which branch of the U.S. military may align with their goals, preferences, and motivations. It outlines how AI tools were used during development, why the product’s AI feature is intentionally scoped the way it is, the risks and trade-offs considered, and the lessons learned about building responsibly with generative AI. This memo reflects the actual practices, judgment calls, and constraints that shaped the final product.

---

## **1. How I Actually Used AI While Building**

AI supported nearly every stage of this project’s development: early ideation, writing structured documents, generating interface code, refactoring components, and debugging HTML and JavaScript issues. I relied primarily on ChatGPT and GitHub Copilot, using them as creative partners and accelerators—not as autonomous builders.

In the earliest stage, AI helped me brainstorm interpretations of the problem and potential product directions. I asked AI to outline scenarios such as: *Should the tool give a direct recommendation? Should it provide comparisons? Should it function as a reflection tool?* This helped me rapidly explore multiple conceptual paths.

When drafting the PRD, README, and documentation, AI generated skeletons and first-pass drafts. These drafts often needed substantial editing, but they eliminated the blank-page problem and gave me structure to refine. I generated multiple versions of each section, then manually combined the strongest elements.

On the technical side, AI provided usable starter code for `index.html`, including forms, labels, section containers, and general layout. GitHub Copilot supported repetitive UI tasks, such as creating event listeners, assembling CSS patterns, or generating boilerplate components. When bugs arose—especially DOM misreferences, formatting issues, or variable scoping problems—I used AI to walk through error messages and propose targeted fixes.

However, AI-generated outputs related to military information often contained subtle inaccuracies, outdated wording, or overly definitive statements. I manually corrected these to ensure the product remained neutral, factual, and ethically responsible. Human editing was especially important in refining tone, aligning explanations with real military norms, and ensuring no branch was implicitly recommended or portrayed inaccurately.

AI accelerated development significantly, but human judgment controlled direction, nuance, safeguards, and the integrity of the final product.

---

## **2. Why the AI Feature in This Product Looks the Way It Does**

The product uses a single AI feature: a personalized narrative summarizing how a user’s stated preferences may align with different military branches. This feature was chosen because military decision-making is complex and multidimensional. Preferences such as mission interest, lifestyle expectations, tolerance for risk, desire for technical roles, or preference for structure cannot be matched well through static logic alone.

Generative AI is uniquely capable of taking multiple qualitative inputs and producing a tailored, human-readable explanation. But I deliberately avoided building anything that resembles a deterministic “military branch recommendation engine.” Instead, the AI produces neutral reflections, highlights trade-offs, and encourages additional research.

This scope was intentional for three reasons:

1. **Ethics** — Generative AI should *not* tell users what branch they should join. Military service is consequential and highly personal.  
2. **Practicality** — A more complex engine would require sensitive data, historical trends, and extensive verification.  
3. **Clarity of Value** — Personalized explanations are impactful without overstepping into advisory territory.

Technically, the feature is lightweight: everything is processed per session, no user data is stored, and the model’s output is framed as informational, not authoritative. This keeps the system transparent, safe, and easy to maintain.

The AI feature directly supports the project’s value proposition: helping a user think more clearly about their interests while preserving autonomy and responsibility.

---

## **3. Risks, Trade-offs, and Integrity**

Because this project touches on real-life military career decisions, ethical considerations guided every design choice.

### **Privacy and Data Use**
The system does not store personal data. It only processes non-identifying preference inputs and avoids logging, tracking, or profiling users. This protects user confidentiality, particularly given the sensitivity of military interest.

### **Bias and Fairness**
Generative AI can unintentionally reinforce stereotypes. To address this:
- Prompts forbid demographic assumptions,
- Outputs were manually inspected for bias,
- Language implying superiority of any branch was removed,
- The model was constrained to avoid making promises about outcomes, careers, or deployment.

### **Over-reliance and User Trust**
The tool repeatedly clarifies that:
- It is *not* an authority,
- It provides reflection, not recommendations,
- Users should consult real service members, mentors, and recruiters.

### **Academic Integrity**
I openly acknowledge AI’s role in drafting portions of this project while ensuring all final reasoning, architecture, constraints, and ethical framing were authored or heavily revised by me. AI did not make core decisions—it generated drafts that were then shaped through human oversight.

Across all stages, intentional limitations prevented the AI from producing misleading, prescriptive, or overconfident statements.

---

## **4. What I Learned About Building with GenAI**

This project taught me that AI is exceptional at generating structure quickly but often unreliable in details that require nuance or domain sensitivity. Drafts came together fast, but refining them into accurate, balanced, and ethically safe content took significant human editing.

I also learned that the quality of output correlates directly with the specificity of prompts. Good prompts produced clear, balanced explanations; vague prompts led to overconfident and occasionally misleading content. This reinforced the principle that AI is only as safe and effective as the constraints and context given to it.

If I were advising another builder, I would emphasize that AI is best used as:
- A brainstorming partner,
- A prototyping accelerator,
- A debugging assistant.

It should not make strategic or ethical decisions. AI magnifies the clarity of human intent—both good and bad. The responsibility lies in thoughtful, deliberate prompt engineering and the courage to revise or reject outputs that do not meet ethical standards.

This project also reshaped how I think about using AI in future work: AI features need as much intentional design as traditional product elements. Responsible AI integration is not merely about what you *can* build, but what you *should* build.

---

## **Conclusion**

Building this project deepened my understanding of the opportunities and responsibilities involved in using generative AI. I learned how to balance speed with oversight, how to constrain an AI to protect users, and how to ensure that AI enhances rather than replaces human judgment. These insights will guide my approach to AI in future academic work and professional projects, especially in areas where decisions carry personal and real-world consequences.

